{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c28c60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#\n",
    "# All the needed data loading helper functions\n",
    "# and libs.\n",
    "#\n",
    "####################################################\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import unicodedata\n",
    "import six\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "def glove2dict(src_filename):\n",
    "    \"\"\"\n",
    "    GloVe vectors file reader.\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_filename : str\n",
    "        Full path to the GloVe file to be processed.\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Mapping words to their GloVe vectors as `np.array`.\n",
    "    \"\"\"\n",
    "    # This distribution has some words with spaces, so we have to\n",
    "    # assume its dimensionality and parse out the lines specially:\n",
    "    if '840B.300d' in src_filename:\n",
    "        line_parser = lambda line: line.rsplit(\" \", 300)\n",
    "    else:\n",
    "        line_parser = lambda line: line.strip().split()\n",
    "    data = {}\n",
    "    with open(src_filename, encoding='utf8') as f:\n",
    "        while True:\n",
    "            try:\n",
    "                line = next(f)\n",
    "                line = line_parser(line)\n",
    "                data[line[0]] = np.array(line[1: ], dtype=np.float64)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                pass\n",
    "    return data\n",
    "\n",
    "\n",
    "def convert_to_unicode(text):\n",
    "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
    "    if six.PY3:\n",
    "        if isinstance(text, str):\n",
    "            return text\n",
    "        elif isinstance(text, bytes):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    elif six.PY2:\n",
    "        if isinstance(text, str):\n",
    "            return text.decode(\"utf-8\", \"ignore\")\n",
    "        elif isinstance(text, unicode):\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
    "    else:\n",
    "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
    "    ids = []\n",
    "    for token in tokens:\n",
    "        if token not in vocab.keys():\n",
    "            ids.append(vocab['[UNK]'])\n",
    "        else:\n",
    "            ids.append(vocab[token])\n",
    "    return ids\n",
    "        \n",
    "def load_vocab(vocab_file):\n",
    "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
    "    vocab = collections.OrderedDict()\n",
    "    index = 0\n",
    "    with open(vocab_file, \"r\") as reader:\n",
    "        while True:\n",
    "            token = convert_to_unicode(reader.readline())\n",
    "            if not token:\n",
    "                break\n",
    "            token = token.strip()\n",
    "            vocab[token] = index\n",
    "            index += 1\n",
    "    return vocab\n",
    "\n",
    "class WordLevelTokenizer(object):\n",
    "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
    "    def __init__(self, vocab_file, config, delimiter=\" \", max_seq_len=128):\n",
    "        self.vocab = load_vocab(vocab_file)\n",
    "        self.vocab_reverse = collections.OrderedDict()\n",
    "        for k, v in self.vocab.items():\n",
    "            self.vocab_reverse[v] = k\n",
    "        self.pad_token_id = config.pad_token_id\n",
    "        self.bos_token_id = config.bos_token_id\n",
    "        self.eos_token_id = config.eos_token_id\n",
    "        self.unk_token_id = config.unk_token_id\n",
    "        self.mask_token_id = config.mask_token_id\n",
    "        self.special_token_ids = set(\n",
    "            [config.pad_token_id, config.bos_token_id, config.eos_token_id, \n",
    "            config.unk_token_id, config.mask_token_id]\n",
    "        )\n",
    "        \n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.delimiter = delimiter\n",
    "        \n",
    "    def tokenize(self, text):\n",
    "        split_tokens = []\n",
    "        for token in text.split(self.delimiter):\n",
    "            split_tokens.append(token)\n",
    "        return split_tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return convert_tokens_to_ids(self.vocab, tokens)\n",
    "    \n",
    "    def __call__(self, text):\n",
    "        original = self.convert_tokens_to_ids(self.tokenize(text))\n",
    "        original = original[:(self.max_seq_len-2)]\n",
    "        return [self.bos_token_id] + original + [self.eos_token_id]\n",
    "    \n",
    "    def batch_decode(self, pred_labels, skip_special_tokens=True):\n",
    "        decode_labels_batch = []\n",
    "        for labels in pred_labels:\n",
    "            decode_labels = []\n",
    "            for l in labels.tolist():\n",
    "                if l == self.eos_token_id:\n",
    "                    break\n",
    "                if l not in self.special_token_ids:\n",
    "                    decode_labels += [self.vocab_reverse[l]]\n",
    "            decode_labels_batch += [self.delimiter.join(decode_labels)]\n",
    "        return decode_labels_batch\n",
    "\n",
    "class COGSDataset(Dataset):\n",
    "    def __init__(\n",
    "        self, cogs_path, \n",
    "        src_tokenizer, tgt_tokenizer,\n",
    "        partition, max_len=512, max_examples=-1,\n",
    "        least_to_most=False\n",
    "    ):\n",
    "        self._items = [] # ()\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "    \n",
    "        self.eval_cat = []\n",
    "        is_gen_dev = False\n",
    "        if partition == \"gen-dev\":\n",
    "            partition = \"gen\"\n",
    "            is_gen_dev = True\n",
    "        for l in open(f\"{cogs_path}/{partition}.tsv\", \"r\").readlines():\n",
    "            if max_examples != -1 and len(self._items) > max_examples:\n",
    "                break\n",
    "            text, sparse, cat = l.split(\"\\t\")\n",
    "            src_input_ids = src_tokenizer(text)\n",
    "            tgt_input_ids = tgt_tokenizer(sparse)\n",
    "            self._items += [(src_input_ids, tgt_input_ids)]\n",
    "            self.eval_cat += [cat.strip()]\n",
    "            \n",
    "        if \"train\" in partition:\n",
    "            random.shuffle(self._items)\n",
    "            if least_to_most:\n",
    "                self._items = sorted(\n",
    "                    self._items, key = lambda i: len(i[0]), \n",
    "                    reverse=False\n",
    "                )\n",
    "\n",
    "        if is_gen_dev:\n",
    "            # this is a strange partition accordingly to previous works.\n",
    "            # well, since other ppl are using this, i have to do it as well!\n",
    "            random.shuffle(self._items)\n",
    "            self._items = sorted(\n",
    "                self._items, key = lambda i: len(i[0]), \n",
    "                reverse=True if not least_to_most else False\n",
    "            )\n",
    "            self._items = self._items[:len(self._items)//10]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self._items)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self._items[item]\n",
    "    \n",
    "    def collate_batch(self, batch):\n",
    "        src_seq_lens = []\n",
    "        tgt_seq_lens = []\n",
    "        for i in range(len(batch)):\n",
    "            src_seq_lens += [len(batch[i][0])]\n",
    "            tgt_seq_lens += [len(batch[i][1])]\n",
    "        max_src_seq_lens = max(src_seq_lens)\n",
    "        max_tgt_seq_lens = max(tgt_seq_lens)\n",
    "        \n",
    "        input_ids_batch = []\n",
    "        mask_batch = []\n",
    "        labels_batch = []\n",
    "        for i in range(len(batch)):\n",
    "            input_ids = batch[i][0] + [0] * (max_src_seq_lens - src_seq_lens[i])\n",
    "            input_ids_batch += [input_ids]\n",
    "            \n",
    "            mask = [1] * src_seq_lens[i] + [0] * (max_src_seq_lens - src_seq_lens[i])\n",
    "            mask_batch += [mask]\n",
    "            \n",
    "            labels = batch[i][1] + [0] * (max_tgt_seq_lens - tgt_seq_lens[i])\n",
    "            labels_batch += [labels]\n",
    "\n",
    "        return {\"input_ids\": torch.tensor(input_ids_batch),\n",
    "                \"labels\": torch.tensor(labels_batch),\n",
    "                \"attention_mask\": torch.tensor(mask_batch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fff62da",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "#\n",
    "# All the needed evaluation related helper functions\n",
    "# and libs.\n",
    "#\n",
    "####################################################\n",
    "import random, torch, re, os\n",
    "import numpy as np\n",
    "from transformers import EncoderDecoderConfig, EncoderDecoderModel\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader, SequentialSampler\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def find_partition_name(name, lf):\n",
    "    if lf == \"cogs\":\n",
    "        return name\n",
    "    else:\n",
    "        return name+f\"_{lf}\"\n",
    "    \n",
    "def check_equal(left_lf, right_lf):\n",
    "    index_mapping = {}\n",
    "    current_idx = 0\n",
    "    for t in left_lf.split():\n",
    "        if t.isnumeric():\n",
    "            if int(t) not in index_mapping:\n",
    "                index_mapping[int(t)] = current_idx\n",
    "                current_idx += 1\n",
    "    decoded_labels_ii = []\n",
    "    for t in left_lf.split():\n",
    "        if t.isnumeric():\n",
    "            decoded_labels_ii += [str(index_mapping[int(t)])]\n",
    "        else:\n",
    "            decoded_labels_ii += [t]\n",
    "\n",
    "    index_mapping = {}\n",
    "    current_idx = 0\n",
    "    for t in right_lf.split():\n",
    "        if t.isnumeric():\n",
    "            if int(t) not in index_mapping:\n",
    "                index_mapping[int(t)] = current_idx\n",
    "                current_idx += 1\n",
    "    decoded_preds_ii = []\n",
    "    for t in right_lf.split():\n",
    "        if t.isnumeric():\n",
    "            decoded_preds_ii += [str(index_mapping[int(t)])]\n",
    "        else:\n",
    "            decoded_preds_ii += [t]\n",
    "\n",
    "\n",
    "    decoded_labels_ii_str = \" \".join(decoded_labels_ii)\n",
    "    decoded_preds_ii_str = \" \".join(decoded_preds_ii)\n",
    "\n",
    "    if decoded_preds_ii_str == decoded_labels_ii_str:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "recogs_neoD_np_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\*)?\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "recogs_neoD_verb_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*([0-9]+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "recogs_neoD_pred_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    ,\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "recogs_neoD_mod_re = re.compile(r\"\"\"\n",
    "    ^\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\.\n",
    "    \\s*(\\w+?)\\s*\n",
    "    \\(\n",
    "    \\s*(.+?)\\s*\n",
    "    ,\n",
    "    \\s*(.+?)\\s*\n",
    "    \\)\n",
    "    \\s*$\"\"\", re.VERBOSE)\n",
    "\n",
    "def translate_invariant_form_neoD(lf):\n",
    "    nouns = lf.split(\" AND \")[0].split(\" ; \")[:-1]\n",
    "    complements = set(lf.split(\" ; \")[-1].split())\n",
    "    nouns_map = {}\n",
    "    new_var = 0\n",
    "    for noun in nouns:\n",
    "        # check format.\n",
    "        if not recogs_neoD_np_re.search(noun):\n",
    "            return {} # this is format error, we cascade the error.\n",
    "        _, _, original_var = recogs_neoD_np_re.search(noun).groups()\n",
    "        if original_var not in complements:\n",
    "            return {} # var must be used, we cascade the error.\n",
    "        new_noun = noun.replace(str(original_var), str(new_var))\n",
    "        nouns_map[original_var] = new_noun\n",
    "        new_var += 1\n",
    "        \n",
    "    nmod_conjs_set = set([])\n",
    "    conjs = lf.split(\" ; \")[-1].split(\" AND \")\n",
    "    vp_conjs_map = {}\n",
    "    nested_conjs = []\n",
    "    childen_count_map = {}\n",
    "    for conj in conjs:\n",
    "        if \"nmod\" in conj:\n",
    "            if not recogs_neoD_mod_re.search(conj):\n",
    "                return {} # this is format error, we cascade the error.\n",
    "            role, pred, first_arg, second_arg = recogs_neoD_mod_re.search(conj).groups()\n",
    "            new_conj = f\"{role} . {pred} ( {nouns_map[first_arg]} , {nouns_map[second_arg]} )\"\n",
    "            nmod_conjs_set.add(new_conj)\n",
    "        else:\n",
    "            if recogs_neoD_verb_re.search(conj):\n",
    "                # candidate for mapping verb.\n",
    "                pred, arg = recogs_neoD_verb_re.search(conj).groups()\n",
    "                if not arg.isnumeric():\n",
    "                    return {}\n",
    "                new_conj = f\"{pred}\"\n",
    "                if arg in vp_conjs_map:\n",
    "                    vp_conjs_map[arg].append(new_conj)\n",
    "                else:\n",
    "                    vp_conjs_map[arg] = [new_conj]\n",
    "                continue\n",
    "            if not recogs_neoD_pred_re.search(conj):\n",
    "                return {} # this is format error, we cascade the error.\n",
    "            \n",
    "            role, first_arg, second_arg = recogs_neoD_pred_re.search(conj).groups()\n",
    "            if first_arg == second_arg or first_arg in nouns_map or not first_arg.isnumeric():\n",
    "                return {} # this is index collision, we cascade the error.\n",
    "            if second_arg.isnumeric() and second_arg in nouns_map:\n",
    "                second_arg = nouns_map[second_arg]\n",
    "                new_conj = f\"{role} ( {second_arg} )\"\n",
    "                if first_arg in vp_conjs_map:\n",
    "                    vp_conjs_map[first_arg].append(new_conj)\n",
    "                else:\n",
    "                    vp_conjs_map[first_arg] = [new_conj]\n",
    "            elif second_arg.isnumeric():\n",
    "                if first_arg not in childen_count_map:\n",
    "                    childen_count_map[first_arg] = 1\n",
    "                else:\n",
    "                    childen_count_map[first_arg] += 1\n",
    "                nested_conjs.append({\n",
    "                    \"role\": role,\n",
    "                    \"first_arg\": first_arg,\n",
    "                    \"second_arg\": second_arg,\n",
    "                })\n",
    "            else:\n",
    "                return {}\n",
    "    \n",
    "    while_loop_count = 0\n",
    "    while len(nested_conjs) > 0:\n",
    "        while_loop_count += 1\n",
    "        if while_loop_count > 100:\n",
    "            return {}\n",
    "        conj = nested_conjs.pop(0)\n",
    "        if conj['second_arg'] not in childen_count_map or childen_count_map[conj['second_arg']] == 0:\n",
    "            core = \" AND \".join(vp_conjs_map[conj['second_arg']])\n",
    "            vp_conjs_map[conj['first_arg']].append(f\"{conj['role']} ( {core} )\")\n",
    "            childen_count_map[conj['first_arg']] -= 1\n",
    "        else:\n",
    "            # if the conj is corrupted, then we abandon just let it go and fail to compare.\n",
    "            if conj['first_arg'] == conj['second_arg']:\n",
    "                return {}\n",
    "            nested_conjs.append(conj)\n",
    "    \n",
    "    filtered_conjs_set = set([])\n",
    "    for k, v in vp_conjs_map.items():\n",
    "        vp_conjs_map[k].sort()\n",
    "    for k, v in vp_conjs_map.items():\n",
    "        vp_expression = \" AND \".join(v)\n",
    "        if vp_expression in filtered_conjs_set:\n",
    "            return {} # this is not allowed. exact same VP expression is not allowed this time.\n",
    "        filtered_conjs_set.add(vp_expression)\n",
    "    for conj in nmod_conjs_set:\n",
    "        if conj in filtered_conjs_set:\n",
    "            return {} # this is not allowed. exact same VP expression is not allowed this time.\n",
    "        filtered_conjs_set.add(conj)\n",
    "    return filtered_conjs_set\n",
    "\n",
    "def check_set_equal_neoD(left_lf, right_lf):\n",
    "    try:\n",
    "        if translate_invariant_form_neoD(left_lf) == \\\n",
    "        translate_invariant_form_neoD(right_lf):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83035aa8",
   "metadata": {},
   "source": [
    "#### HF Model Loading with Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a12fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME = \"ReCOGS\" # COGS or ReCOGS\n",
    "####################################################\n",
    "#\n",
    "# Different evaluation function for COGS and ReCOGS\n",
    "#\n",
    "####################################################\n",
    "exact_match_func = check_equal if DATASET_NAME == \"COGS\" else check_set_equal_neoD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408363eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderDecoderModel.from_pretrained(\n",
    "    f\"ReCOGS/{DATASET_NAME}-model\",\n",
    "    cache_dir=\"../huggingface_cache/\"\n",
    ")\n",
    "_ = model.to(device)\n",
    "_ = model.eval()\n",
    "set_seed(123) # should be seed invariant, but just to make sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57ecfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading tokenizers and datasets\n",
    "model_data_path = \"./model/\"\n",
    "data_path = \"./cogs/\" if DATASET_NAME == \"COGS\" else \"./recogs/\"\n",
    "using_set_equal = False if DATASET_NAME == \"COGS\" else True\n",
    "src_tokenizer = WordLevelTokenizer(\n",
    "    os.path.join(model_data_path, \"src_vocab.txt\"), \n",
    "    model.config.encoder,\n",
    "    max_seq_len=512\n",
    ")\n",
    "tgt_tokenizer = WordLevelTokenizer(\n",
    "    os.path.join(model_data_path, \"tgt_vocab.txt\"), \n",
    "    model.config.decoder,\n",
    "    max_seq_len=512\n",
    ")\n",
    "\n",
    "test_dataset = COGSDataset(\n",
    "    cogs_path=data_path, \n",
    "    src_tokenizer=src_tokenizer, \n",
    "    tgt_tokenizer=tgt_tokenizer, \n",
    "    partition=find_partition_name(\"test\", \"cogs\"),\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset, batch_size=256, \n",
    "    sampler=SequentialSampler(test_dataset),\n",
    "    collate_fn=test_dataset.collate_batch\n",
    ")\n",
    "\n",
    "gen_dataset = COGSDataset(\n",
    "    cogs_path=data_path, \n",
    "    src_tokenizer=src_tokenizer, \n",
    "    tgt_tokenizer=tgt_tokenizer, \n",
    "    partition=find_partition_name(\"gen\", \"cogs\"),\n",
    ")\n",
    "gen_dataloader = DataLoader(\n",
    "    gen_dataset, batch_size=256, \n",
    "    sampler=SequentialSampler(gen_dataset),\n",
    "    collate_fn=test_dataset.collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a71c19",
   "metadata": {},
   "source": [
    "IID Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cec385",
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_iterator = tqdm(test_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for step, inputs in enumerate(epoch_iterator):\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    labels = inputs[\"labels\"].to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        max_length=512,\n",
    "    )\n",
    "    decoded_preds = tgt_tokenizer.batch_decode(outputs)\n",
    "    decoded_labels = tgt_tokenizer.batch_decode(labels)\n",
    "\n",
    "    for i in range(len(decoded_preds)):\n",
    "        if exact_match_func(decoded_labels[i], decoded_preds[i]):\n",
    "            correct_count += 1\n",
    "        else:\n",
    "            pass\n",
    "        total_count += 1\n",
    "    current_acc = round(correct_count/total_count, 2)\n",
    "    epoch_iterator.set_postfix({'acc': current_acc})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0eb2ea",
   "metadata": {},
   "source": [
    "OOD Test Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "per_cat_eval = {}\n",
    "for cat in set(gen_dataset.eval_cat):\n",
    "    per_cat_eval[cat] = [0, 0] # correct, total\n",
    "\n",
    "epoch_iterator = tqdm(gen_dataloader, desc=\"Iteration\", position=0, leave=True)\n",
    "total_count = 0\n",
    "correct_count = 0\n",
    "for step, inputs in enumerate(epoch_iterator):\n",
    "    input_ids = inputs[\"input_ids\"].to(device)\n",
    "    attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "    labels = inputs[\"labels\"].to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        eos_token_id=model.config.eos_token_id,\n",
    "        max_length=512,\n",
    "    )\n",
    "    decoded_preds = tgt_tokenizer.batch_decode(outputs)\n",
    "    decoded_labels = tgt_tokenizer.batch_decode(labels)\n",
    "\n",
    "    input_labels = src_tokenizer.batch_decode(input_ids)\n",
    "    for i in range(len(decoded_preds)):\n",
    "        cat = gen_dataset.eval_cat[total_count]\n",
    "        if exact_match_func(decoded_labels[i], decoded_preds[i]):\n",
    "            correct_count += 1\n",
    "            per_cat_eval[cat][0] += 1\n",
    "        else:\n",
    "            pass\n",
    "        total_count += 1\n",
    "        per_cat_eval[cat][1] += 1\n",
    "    current_acc = correct_count/total_count\n",
    "    epoch_iterator.set_postfix({'acc': current_acc})\n",
    "\n",
    "struct_pp_acc = 0\n",
    "struct_cp_acc = 0\n",
    "struct_obj_subj_acc = 0\n",
    "\n",
    "lex_acc = 0\n",
    "lex_count = 0\n",
    "for k, v in per_cat_eval.items():\n",
    "    if k  == \"pp_recursion\":\n",
    "        struct_pp_acc = 100 * v[0]/v[1]\n",
    "    elif k  == \"cp_recursion\":\n",
    "        struct_cp_acc = 100 * v[0]/v[1]\n",
    "    elif k  == \"obj_pp_to_subj_pp\":\n",
    "        struct_obj_subj_acc = 100 * v[0]/v[1]\n",
    "    elif k  == \"subj_to_obj_proper\":\n",
    "        subj_to_obj_proper_acc = 100 * v[0]/v[1]\n",
    "    elif k  == \"prim_to_obj_proper\":\n",
    "        prim_to_obj_proper_acc = 100 * v[0]/v[1]\n",
    "    elif k  == \"prim_to_subj_proper\": \n",
    "        prim_to_subj_proper_acc = 100 * v[0]/v[1]\n",
    "    else:\n",
    "        lex_acc += v[0]\n",
    "        lex_count += v[1]\n",
    "lex_acc /= lex_count\n",
    "lex_acc *= 100\n",
    "current_acc *= 100\n",
    "\n",
    "print(f\"obj_pp_to_subj_pp: {struct_obj_subj_acc}\")\n",
    "print(f\"cp_recursion: {struct_cp_acc}\")\n",
    "print(f\"pp_recursion: {struct_pp_acc}\")\n",
    "print(f\"subj_to_obj_proper: {subj_to_obj_proper_acc}\")\n",
    "print(f\"prim_to_obj_proper: {prim_to_obj_proper_acc}\")\n",
    "print(f\"prim_to_subj_proper: {prim_to_subj_proper_acc}\")\n",
    "print(f\"LEX: {lex_acc}\")\n",
    "print(f\"OVERALL: {current_acc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
