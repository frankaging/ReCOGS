{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab4efe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.second_looks_utils import *\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a44d37",
   "metadata": {},
   "source": [
    "## Our reformatting functions\n",
    "- Redundant token removal\n",
    "- Example concatenation for longer training sequences\n",
    "- Preposing\n",
    "- Preposing + Interjection\n",
    "- ReCOGS\n",
    "- Variable-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "9eca50b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_invariant_form(lf):\n",
    "    nouns = lf.split(\" AND \")[0].split(\" ; \")[:-1]\n",
    "    nouns_map = {}\n",
    "    new_var = 0\n",
    "    for noun in nouns:\n",
    "        original_var = noun.split()[-2]\n",
    "        new_noun = noun.replace(str(original_var), str(new_var))\n",
    "        nouns_map[original_var] = new_noun\n",
    "        new_var += 1\n",
    "    \n",
    "    conjs_set = set([])\n",
    "    conjs = lf.split(\" ; \")[-1].split(\" AND \")\n",
    "    vp_conjs_map = {}\n",
    "    nested_conjs = []\n",
    "    childen_count_map = {}\n",
    "    for conj in conjs:\n",
    "        if \"nmod\" in conj:\n",
    "            role = conj.split()[0]\n",
    "            pred = conj.split()[2]\n",
    "            first_arg = conj.split()[-4]\n",
    "            second_arg = conj.split()[-2]\n",
    "            new_conj = f\"{role} . {pred} ( {nouns_map[first_arg]} , {nouns_map[second_arg]} )\"\n",
    "            conjs_set.add(new_conj)\n",
    "        else:\n",
    "            role = conj.split()[0]\n",
    "            pred = conj.split()[2]\n",
    "            first_arg = conj.split()[-4]\n",
    "            second_arg = conj.split()[-2]\n",
    "            if first_arg == second_arg:\n",
    "                continue # this is wrong, we dont decode, let error casacade\n",
    "            if second_arg.isnumeric() and second_arg in nouns_map:\n",
    "                second_arg = nouns_map[second_arg]\n",
    "                new_conj = f\"{role} . {pred} ( {second_arg} )\"\n",
    "                if first_arg in vp_conjs_map:\n",
    "                    vp_conjs_map[first_arg].append(new_conj)\n",
    "                else:\n",
    "                    vp_conjs_map[first_arg] = [new_conj]\n",
    "            elif second_arg.isnumeric():\n",
    "                if first_arg not in childen_count_map:\n",
    "                    childen_count_map[first_arg] = 1\n",
    "                else:\n",
    "                    childen_count_map[first_arg] += 1\n",
    "                nested_conjs.append({\n",
    "                    \"pred\": pred,\n",
    "                    \"role\": role,\n",
    "                    \"first_arg\": first_arg,\n",
    "                    \"second_arg\": second_arg,\n",
    "                })\n",
    "            else:\n",
    "                new_conj = f\"{role} . {pred} ( {second_arg} )\"\n",
    "                if first_arg in vp_conjs_map:\n",
    "                    vp_conjs_map[first_arg].append(new_conj)\n",
    "                else:\n",
    "                    vp_conjs_map[first_arg] = [new_conj]\n",
    "                    \n",
    "    print(vp_conjs_map)\n",
    "    print(nested_conjs)\n",
    "    print(childen_count_map)\n",
    "    \n",
    "    while_loop_count = 0\n",
    "    while len(nested_conjs) > 0:\n",
    "        while_loop_count += 1\n",
    "        if while_loop_count > 100:\n",
    "            break # no need, this answer has to be wrong!\n",
    "        conj = nested_conjs.pop(0)\n",
    "        if conj['second_arg'] not in childen_count_map or childen_count_map[conj['second_arg']] == 0:\n",
    "            core = \" AND \".join(vp_conjs_map[conj['second_arg']])\n",
    "            vp_conjs_map[conj['first_arg']].append(f\"{conj['role']} . {conj['pred']} ( {core} )\")\n",
    "            childen_count_map[conj['first_arg']] -= 1\n",
    "        else:\n",
    "            # if the conj is corrupted, then we abandon just let it go and fail to compare.\n",
    "            if conj['first_arg'] == conj['second_arg']:\n",
    "                continue\n",
    "            nested_conjs.append(conj)\n",
    "            \n",
    "    print(vp_conjs_map)\n",
    "    print(nested_conjs)\n",
    "    print(childen_count_map)\n",
    "    print(while_loop_count)\n",
    "    \n",
    "    filtered_conjs_set = set([])\n",
    "    for k, v in vp_conjs_map.items():\n",
    "        vp_conjs_map[k].sort()\n",
    "    for k, v in vp_conjs_map.items():\n",
    "        filtered_conjs_set.add(\" AND \".join(v))\n",
    "    \n",
    "    return filtered_conjs_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "9483735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'18': ['agent . give ( girl ( 0 ) )', 'recipient . give ( James )']}\n",
      "[]\n",
      "{}\n",
      "{'18': ['agent . give ( girl ( 0 ) )', 'recipient . give ( James )']}\n",
      "[]\n",
      "{}\n",
      "0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'agent . give ( girl ( 0 ) ) AND recipient . give ( James )'}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_invariant_form(\n",
    "\"girl ( 41 ) ; donut ( 18 ) ; agent . give ( 18 , 41 ) AND recipient . give ( 18 , James ) AND theme . give ( 18 , 18 )\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "b406d484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'agent . imagine ( boy ( 0 ) ) AND ccomp . imagine ( agent . wire ( * teacher ( 1 ) ) AND recipient . wire ( Emma ) AND theme . wire ( * cake ( 2 ) ) )',\n",
       " 'agent . wire ( * teacher ( 1 ) ) AND recipient . wire ( Emma ) AND theme . wire ( * cake ( 2 ) )'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_invariant_form(\n",
    "\"boy ( 23 ) ; * teacher ( 9 ) ; * cake ( 38 ) ; agent . imagine ( 47 , 23 ) AND ccomp . imagine ( 47 , 50 ) AND agent . wire ( 50 , 9 ) AND recipient . wire ( 50 , Emma ) AND theme . wire ( 50 , 38 )\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c03764",
   "metadata": {},
   "source": [
    "### Redundant Token Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80dda3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for removing_set in [\n",
    "    ['x', '_'],\n",
    "    ['x', '_', '(', ')'],\n",
    "    ['x', '_', '(', ',', ')']\n",
    "]:\n",
    "    def token_removal(text, phi): \n",
    "        # Parsing:\n",
    "        terms = []\n",
    "        for t in phi.split():\n",
    "            if t not in removing_set:\n",
    "                terms += [t]\n",
    "        ret = \" \".join(terms).strip()\n",
    "        return ret\n",
    "    train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "    train_df['LF'] = train_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "    dev_df['LF'] = dev_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "    test_df['LF'] = test_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "    gen_df['LF'] = gen_df[['sentence', 'LF']].apply(lambda x: token_removal(*x), axis=1,)\n",
    "    \n",
    "    removing_set_str = \"\".join(removing_set)\n",
    "    dataset_postfix = f\"remove_{removing_set_str}\"\n",
    "    train_df.to_csv(f'./cogs_token_removal/train_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    dev_df.to_csv(f'./cogs_token_removal/dev_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    test_df.to_csv(f'./cogs_token_removal/test_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    gen_df.to_csv(f'./cogs_token_removal/gen_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c1d099",
   "metadata": {},
   "source": [
    "### Example Concatenations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "92cabc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(LFs, initial_indexes):\n",
    "    new_LF_prefix = []\n",
    "    new_LF_body = []\n",
    "    for i in range(len(LFs)):\n",
    "        if initial_indexes[i] != 0:\n",
    "            new_lf = []\n",
    "            for item in LFs[i].split():\n",
    "                if item.isnumeric():\n",
    "                    new_i = int(item) + initial_indexes[i]\n",
    "                    new_lf += [str(new_i)]\n",
    "                else:\n",
    "                    new_lf += [item]\n",
    "            new_lf = \" \".join(new_lf)\n",
    "        else:\n",
    "            new_lf = LFs[i]\n",
    "        \n",
    "        for item in new_lf.split(\" ; \"):\n",
    "            if \"*\" in item:\n",
    "                new_LF_prefix += [item]\n",
    "            else:\n",
    "                new_LF_body += [item]\n",
    "        new_LF_body += [\"AND\"]\n",
    "    new_LF_body = new_LF_body[:-1]\n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" \".join(new_LF_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7785a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "append_ks = [256, 512, 1024, 2048, 3072]\n",
    "for append_k in append_ks:\n",
    "    train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    train_df_org = train_df.copy()\n",
    "    train_df = train_df[train_df[\"type\"] != \"primitive\"]\n",
    "    dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    dataset_postfix = f\"k_{append_k}\"\n",
    "    append_data = []\n",
    "    start_indexes = [i*6 for i in range(append_k)]\n",
    "    sorted_train_df = train_df.sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        conj_1 = sorted_train_df.iloc[-2-start_index].sentence\n",
    "        if conj_1.split()[0] in {'The', 'A'}:\n",
    "            conj_1_first = conj_1[0].lower()\n",
    "        else:\n",
    "            conj_1_first = conj_1[0]\n",
    "            \n",
    "        conj_2 = sorted_train_df.iloc[-3-start_index].sentence\n",
    "        if conj_2.split()[0] in {'The', 'A'}:\n",
    "            conj_2_first = conj_2[0].lower()\n",
    "        else:\n",
    "            conj_2_first = conj_2[0]\n",
    "            \n",
    "        append_data += [\n",
    "            [sorted_train_df.iloc[-1-start_index].sentence[:-1]+\\\n",
    "            conj_1_first+\\\n",
    "            sorted_train_df.iloc[-2-start_index].sentence[1:-1]+\\\n",
    "            conj_2_first+\\\n",
    "            sorted_train_df.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    sorted_train_df.iloc[-1-start_index].LF,\n",
    "                    sorted_train_df.iloc[-2-start_index].LF,\n",
    "                    sorted_train_df.iloc[-3-start_index].LF\n",
    "                ],\n",
    "                [\n",
    "                    0,\n",
    "                    len(sorted_train_df.iloc[-1-start_index].sentence[:-1].strip().split()),\n",
    "                    len(sorted_train_df.iloc[-1-start_index].sentence[:-1].strip().split())+\n",
    "                    len(sorted_train_df.iloc[-2-start_index].sentence[:-1].strip().split())\n",
    "                ]\n",
    "            ),\n",
    "            'concat']\n",
    "        ]\n",
    "    append_df = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])\n",
    "    train_df = pd.concat([train_df_org, append_df])\n",
    "    train_df.to_csv(f'./cogs_concat/train_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    dev_df.to_csv(f'./cogs_concat/dev_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    test_df.to_csv(f'./cogs_concat/test_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    gen_df.to_csv(f'./cogs_concat/gen_{dataset_postfix}.tsv', sep='\\t', index=False, header=False)\n",
    "    \n",
    "    max_s = max(train_df['sentence'].str.split().apply(len))\n",
    "    max_lf = max(train_df['LF'].str.split().apply(len))\n",
    "    print(max_s, max_lf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5eb6f",
   "metadata": {},
   "source": [
    "### Preposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "845703e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "        \n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if \"nmod\" not in terms.split():\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= proposing_prob:\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if terms.split().count(\"nmod\") == 2:\n",
    "        upper_bound = 6\n",
    "        return_type = \"preposing_2\"\n",
    "    elif terms.split().count(\"nmod\") == 1:\n",
    "        upper_bound = 3\n",
    "        return_type = \"preposing_1\"\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    nmod = terms.split()[terms.split().index(\"nmod\")+2]\n",
    "    pre_phrase = text.split()[text.split().index(nmod)-2 : text.split().index(nmod)+upper_bound]\n",
    "    pre_phrase[0] = pre_phrase[0].capitalize()\n",
    "    pre_text = text.split()[:text.split().index(nmod)-2]\n",
    "    if pre_text[0] in [\"The\", \"A\"]:\n",
    "        pre_text[0] = pre_text[0].lower()\n",
    "    post_text = text.split()[text.split().index(nmod)+upper_bound:]\n",
    "    pre_text = pre_phrase + pre_text + post_text\n",
    "    pre_text = \" \".join(pre_text)\n",
    "\n",
    "    index_map = {}\n",
    "    idx = 0\n",
    "    for i in range(text.split().index(nmod)-2, text.split().index(nmod)+upper_bound):\n",
    "        index_map[f\"{i}\"] = f\"{idx}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)-2):\n",
    "        ii = 2+upper_bound+i\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)+upper_bound, len(text.split())):\n",
    "        ii = idx+(i-(text.split().index(nmod)+upper_bound))\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"  \n",
    "            \n",
    "    # now handle LF\n",
    "    pre_terms = []\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            pre_terms += [str(index_map[str(int(t))])]\n",
    "        else:\n",
    "            pre_terms += [t]\n",
    "    pre_terms = \" \".join(pre_terms)\n",
    "\n",
    "    pre_terms_def = pre_terms.split(\" ; \")[:-1]\n",
    "    pre_terms_def.sort(key = lambda x: int(x.split()[-2]))  \n",
    "    pre_terms_role = pre_terms.split(\" ; \")[-1].split(\" AND \")\n",
    "    pre_terms_role.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else pre_text.split().index(x.split()[-2])))  \n",
    "    pre_terms_role = \" AND \".join(pre_terms_role)\n",
    "    pre_terms = \" ; \".join(pre_terms_def + [pre_terms_role])\n",
    "\n",
    "    return pre_text, pre_terms, return_type\n",
    "\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "proposing_prob = 0.05\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00d26b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"preposing\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96411af9",
   "metadata": {},
   "source": [
    "### Preposing + Sprinkles (Interjection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6f5c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "        \n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if \"nmod\" not in terms.split():\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= proposing_prob:\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if terms.split().count(\"nmod\") == 2:\n",
    "        upper_bound = 6\n",
    "        return_type = \"preposing_2\"\n",
    "    elif terms.split().count(\"nmod\") == 1:\n",
    "        upper_bound = 3\n",
    "        return_type = \"preposing_1\"\n",
    "    else:\n",
    "        assert False\n",
    "        \n",
    "    nmod = terms.split()[terms.split().index(\"nmod\")+2]\n",
    "    pre_phrase = text.split()[text.split().index(nmod)-2 : text.split().index(nmod)+upper_bound]\n",
    "    pre_phrase[0] = pre_phrase[0].capitalize()\n",
    "    pre_text = text.split()[:text.split().index(nmod)-2]\n",
    "    if pre_text[0] in [\"The\", \"A\"]:\n",
    "        pre_text[0] = pre_text[0].lower()\n",
    "    post_text = text.split()[text.split().index(nmod)+upper_bound:]\n",
    "    pre_text = pre_phrase + pre_text + post_text\n",
    "    pre_text = \" \".join(pre_text)\n",
    "\n",
    "    index_map = {}\n",
    "    idx = 0\n",
    "    for i in range(text.split().index(nmod)-2, text.split().index(nmod)+upper_bound):\n",
    "        index_map[f\"{i}\"] = f\"{idx}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)-2):\n",
    "        ii = 2+upper_bound+i\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"\n",
    "        idx += 1\n",
    "    for i in range(text.split().index(nmod)+upper_bound, len(text.split())):\n",
    "        ii = idx+(i-(text.split().index(nmod)+upper_bound))\n",
    "        index_map[f\"{i}\"] = f\"{ii}\"  \n",
    "            \n",
    "    # now handle LF\n",
    "    pre_terms = []\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            pre_terms += [str(index_map[str(int(t))])]\n",
    "        else:\n",
    "            pre_terms += [t]\n",
    "    pre_terms = \" \".join(pre_terms)\n",
    "\n",
    "    pre_terms_def = pre_terms.split(\" ; \")[:-1]\n",
    "    pre_terms_def.sort(key = lambda x: int(x.split()[-2]))  \n",
    "    pre_terms_role = pre_terms.split(\" ; \")[-1].split(\" AND \")\n",
    "    pre_terms_role.sort(key = lambda x: (int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else pre_text.split().index(x.split()[-2])))  \n",
    "    pre_terms_role = \" AND \".join(pre_terms_role)\n",
    "    pre_terms = \" ; \".join(pre_terms_def + [pre_terms_role])\n",
    "\n",
    "    return pre_text, pre_terms, return_type\n",
    "\n",
    "def add_um(sentence):\n",
    "    words = sentence.split()\n",
    "    new_words = []\n",
    "    mapping = {}\n",
    "    offset = 0\n",
    "    for i, word in enumerate(words):\n",
    "        mapping[i] = len(new_words)\n",
    "        new_words.append(word)\n",
    "        if i > 0 and i < len(words) - 2 and random.random() > 0.5:\n",
    "            num_um = random.choice([1,2,3])\n",
    "            for j in range(num_um):\n",
    "                new_words.append(\"um\")\n",
    "    return \" \".join(new_words), mapping\n",
    "\n",
    "def sprinkle(text, phi, _type):\n",
    "    if \"preposition\" in _type:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    if random.random() >= sprinkle_prob:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    um_text, token_mapping = add_um(text)\n",
    "    um_phi = []\n",
    "    for t in phi.split():\n",
    "        if t.isnumeric():\n",
    "            um_phi += [str(token_mapping[int(t)])]\n",
    "        else:\n",
    "            um_phi += [t]\n",
    "    um_phi = \" \".join(um_phi)\n",
    "    \n",
    "    return um_text, um_phi, \"sprinkle\"\n",
    "            \n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "proposing_prob = 0.05\n",
    "sprinkle_prob = 0.05\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(lambda x: sprinkle(*x), axis=1, result_type='expand')\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f2242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"preposing+sprinkles\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d755a5d7",
   "metadata": {},
   "source": [
    "### ReCOGS (Number of resampling iterations = 5)\n",
    "It seems like the performance gain from increasing the number of resampling iterations dimish quickly after getting the number above 10. We are trying 5 here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2541f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "existing_digit_pool = set([])\n",
    "# loading target vocab to random sample our variable names\n",
    "for k, v in load_vocab(\"./model/tgt_vocab.txt\").items():\n",
    "    if k.isnumeric():\n",
    "        existing_digit_pool.add(k)\n",
    "existing_digit_pool = list(existing_digit_pool)\n",
    "\n",
    "def translate(text, phi):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        phi_split = phi.split()\n",
    "        if len(phi_split) == 7:\n",
    "            return text, phi\n",
    "        v_pos = []\n",
    "        idx = 0\n",
    "        for t in phi_split:\n",
    "            if t == text:\n",
    "                v_pos += [idx]\n",
    "            idx += 1\n",
    "        for p in v_pos:\n",
    "            phi_split[p] = phi_split[p+2]\n",
    "            phi_split[p+2] = text\n",
    "        \n",
    "        return text, \" \".join(phi_split)\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                def_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['role']} . {d['pred']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            nmod_terms += [f\"nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = nmod_terms + role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    # final step, remove biases\n",
    "    current_digit_pool = set([])\n",
    "    for t in terms.split():\n",
    "        if t.isnumeric():\n",
    "            current_digit_pool.add(t)\n",
    "    current_digit_pool = list(current_digit_pool)\n",
    "    random.shuffle(current_digit_pool)\n",
    "    sample_random_digit = random.sample(existing_digit_pool, k=len(current_digit_pool))\n",
    "    digit_mapping = dict(zip(current_digit_pool, sample_random_digit))\n",
    "    \n",
    "    new_terms = []\n",
    "    for t in terms.split():\n",
    "        if t == \"_\" or t == \"x\":\n",
    "            continue\n",
    "        if t.isnumeric():\n",
    "            new_terms += [digit_mapping[t]]\n",
    "        else:\n",
    "            new_terms += [t]\n",
    "\n",
    "    terms = \" \".join(new_terms)\n",
    "    return text, terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0e83a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_n = 5\n",
    "append_k = 3072\n",
    "\n",
    "train_dfs = []\n",
    "for i in range(sampled_n):\n",
    "    train_df_i = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "    train_df_i[['sentence', 'LF']] = train_df_i[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "    train_dfs += [train_df_i]\n",
    "    \n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df[['sentence', 'LF']] = dev_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF']] = test_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF']] = gen_df[['sentence', 'LF']].apply(lambda x: translate(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ab36386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reindex(LFs, existing_digit_pool):\n",
    "    curr_digit = set([])\n",
    "    for i in range(len(LFs)):\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                curr_digit.add((i, int(item)))\n",
    "    sampled_digits = random.sample(existing_digit_pool, k=len(curr_digit))\n",
    "    digit_map = {}\n",
    "    idx = 0\n",
    "    for d in list(curr_digit):\n",
    "        digit_map[d] = sampled_digits[idx]\n",
    "        idx += 1\n",
    "    \n",
    "    reindex_LFs = []\n",
    "    for i in range(len(LFs)):\n",
    "        new_LFs = []\n",
    "        for item in LFs[i].split():\n",
    "            if item.isnumeric():\n",
    "                new_LFs += [digit_map[(i, int(item))]]\n",
    "            else:\n",
    "                new_LFs += [item]\n",
    "        reindex_LFs += [\" \".join(new_LFs)]\n",
    "        \n",
    "    new_LF_prefix = []\n",
    "    new_LF_body_nmod = []\n",
    "    new_LF_body_verb = []\n",
    "        \n",
    "    for i in range(len(reindex_LFs)):\n",
    "        new_LF_prefix.extend(reindex_LFs[i].split(\" ; \")[:-1])\n",
    "        for term in reindex_LFs[i].split(\" ; \")[-1].split(\" AND \"):\n",
    "            if \"nmod\" in term:\n",
    "                new_LF_body_nmod += [term]\n",
    "            else:\n",
    "                new_LF_body_verb += [term]\n",
    "                \n",
    "    new_LF_body = new_LF_body_nmod + new_LF_body_verb\n",
    "        \n",
    "    return \" ; \".join(new_LF_prefix) + \" ; \" + \" AND \".join(new_LF_body)\n",
    "\n",
    "start_indexes = [i*6 for i in range(append_k)]\n",
    "append_data = []\n",
    "\n",
    "for i in range(sampled_n):\n",
    "    train_df_sorted = train_dfs[i].sort_values(by=\"sentence\", key=lambda x: x.str.len())\n",
    "    for start_index in start_indexes:\n",
    "        conj_1 = train_df_sorted.iloc[-2-start_index].sentence\n",
    "        if conj_1.split()[0] in {'The', 'A'}:\n",
    "            conj_1_first = conj_1[0].lower()\n",
    "        else:\n",
    "            conj_1_first = conj_1[0]\n",
    "            \n",
    "        conj_2 = train_df_sorted.iloc[-3-start_index].sentence\n",
    "        if conj_2.split()[0] in {'The', 'A'}:\n",
    "            conj_2_first = conj_2[0].lower()\n",
    "        else:\n",
    "            conj_2_first = conj_2[0]\n",
    "            \n",
    "        append_data += [\n",
    "            [train_df_sorted.iloc[-1-start_index].sentence[:-1]+\\\n",
    "            conj_1_first+\\\n",
    "            train_df_sorted.iloc[-2-start_index].sentence[1:-1]+\\\n",
    "            conj_2_first+\\\n",
    "            train_df_sorted.iloc[-3-start_index].sentence[1:],\n",
    "            reindex(\n",
    "                [\n",
    "                    train_df_sorted.iloc[-1-start_index].LF,\n",
    "                    train_df_sorted.iloc[-2-start_index].LF,\n",
    "                    train_df_sorted.iloc[-3-start_index].LF\n",
    "                ], existing_digit_pool\n",
    "            ),\n",
    "            'length_ood']\n",
    "        ]\n",
    "append_data = pd.DataFrame(append_data, columns =['sentence', 'LF', 'type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "471bc3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat(train_dfs)\n",
    "train_df = pd.concat([train_df, append_data])\n",
    "\n",
    "dataset_postfix = \"recogs\"\n",
    "train_df.to_csv(f'./{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3bc584",
   "metadata": {},
   "source": [
    "### Variable-free format\n",
    "This is from Qiu et. al., 2022 paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e28d543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cogs_lf_to_funcall(lf):\n",
    "    \"\"\"Converts the given COGS logical form into the variable-free form.\n",
    "    - Nouns (entities and unaries) become values:\n",
    "      Jack --> Jack\n",
    "      cat ( x _ 1 ) --> cat\n",
    "      * cat ( x _ 1 ) --> * cat\n",
    "    - Verbs become functions, and their roles become argument names:\n",
    "      eat . agent ( x _ 2 , Jack ) --> eat ( agent = Jack )\n",
    "    - The variables representing nouns resolve to their values:\n",
    "      cat ( x _ 1 ) AND eat . agent ( x _ 2 , x _ 1 ) --> eat ( agent = cat )\n",
    "    This converter constructs a graph where variables are nodes and binaries\n",
    "    are edges. After identifying the root, it then performs depth-first traversal\n",
    "    to construct the output.\n",
    "    Args:\n",
    "    lf: Logical form string.\n",
    "    Returns:\n",
    "    The converted logical form.\n",
    "    \"\"\"\n",
    "    if \"LAMBDA\" in lf or \"(\" not in lf:\n",
    "        return lf\n",
    "\n",
    "    # Parse the terms in the logical form\n",
    "    # Example: toss . agent ( x _ 1 , John ) --> [toss, agent], [x _ 1, John]\n",
    "    terms = []\n",
    "    for raw_term in re.split(\" ; | AND \", lf):\n",
    "        match = re.match(r\"(.*) \\( (.*) \\)\", raw_term)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Malformed term: {raw_term}\")\n",
    "        labels = match.group(1).split(\" . \")\n",
    "        args = match.group(2).split(\" , \")\n",
    "        if len(args) not in (1, 2):\n",
    "            raise ValueError(f\"Invalid number of args: {args}\")\n",
    "        terms.append((labels, args))\n",
    "\n",
    "    # `nodes` maps variables to node name (e.g., \"x _ 3\" -> \"* cat\").\n",
    "    nodes = {}\n",
    "    for labels, args in terms:\n",
    "        if args[0] in nodes:\n",
    "            # The variable has already been seen; check for conflicts.\n",
    "            if nodes[args[0]] not in (labels[0], \"* \" + labels[0]):\n",
    "                raise ValueError(\n",
    "                    f\"Conflicting node name: {nodes[args[0]]} vs. {labels[0]}\")\n",
    "        else:\n",
    "            nodes[args[0]] = labels[0]\n",
    "\n",
    "    # `children` maps variables to a list of (edge name, target node).\n",
    "    children = {}\n",
    "    # Potential root nodes; any node being a child will be removed.\n",
    "    root_candidates = list(nodes)\n",
    "    for labels, args in terms:\n",
    "        if len(args) == 2:\n",
    "            if args[0] not in children:\n",
    "                children[args[0]] = []\n",
    "            children[args[0]].append((\" . \".join(labels[1:]), args[1]))\n",
    "            if args[1] in root_candidates:\n",
    "                root_candidates.remove(args[1])\n",
    "    if len(root_candidates) != 1:\n",
    "        raise ValueError(f\"Multiple roots: {root_candidates}\")\n",
    "    root = root_candidates[0]\n",
    "\n",
    "    # Depth-first traverse the graph to construct the funcall\n",
    "    def dfs(node):\n",
    "        if node not in nodes:\n",
    "            # Named entity such as \"John\"\n",
    "            if node.startswith(\"x _\"):\n",
    "                raise ValueError(f\"Unbound variable {node}\")\n",
    "            if node in children:\n",
    "                raise ValueError(f\"Named entity {node} has children {children[node]}\")\n",
    "            return [node]\n",
    "        else:\n",
    "            # A noun like \"cat\" or a verb like \"jump\"\n",
    "            if node not in children:\n",
    "                return [nodes[node]]\n",
    "            funcall_args = []\n",
    "            for edge_label, edge_target in children[node]:\n",
    "                funcall_args.append([edge_label, \"=\"] + dfs(edge_target))\n",
    "            funcall = [nodes[node], \"(\"]\n",
    "            for i, funcall_arg in enumerate(funcall_args):\n",
    "                if i != 0:\n",
    "                    funcall.append(\",\")\n",
    "                funcall.extend(funcall_arg)\n",
    "            funcall.append(\")\")\n",
    "            return funcall\n",
    "\n",
    "    return \" \".join(dfs(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec6c7fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "train_df['LF'] = train_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)\n",
    "dev_df['LF'] = dev_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)\n",
    "test_df['LF'] = test_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)\n",
    "gen_df['LF'] = gen_df[['LF']].apply(lambda x: cogs_lf_to_funcall(*x), axis=1,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "72eb5e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"variable_free\"\n",
    "train_df.to_csv(f'./{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3cd798",
   "metadata": {},
   "source": [
    "### Participial Verb Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4548149",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "noun_pool = set()\n",
    "v_pool = set()\n",
    "def collect_nv(text, phi, _type):\n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    role_set = set([])\n",
    "    for d in data:\n",
    "        if d['type'] == 'role':\n",
    "            v_pool.add(d['pred'])\n",
    "        elif d['type'] == 'np':\n",
    "            noun_pool.add(d['pred'])\n",
    "            \n",
    "    return text, phi, _type\n",
    "\n",
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "\n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    role_set = set([])\n",
    "    object_entvar = None\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "            if d['role'] == \"theme\":\n",
    "                object_entvar = d['entvar']\n",
    "                \n",
    "            role_set.add(d['role'])\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    start_with_pp = False if text_split[0] in {'A', 'The'} else True\n",
    "    \n",
    "    if start_with_pp or (object_entvar is not None and \"x _ \" not in object_entvar):\n",
    "        # combine\n",
    "        def_terms = \" ; \".join(def_terms)\n",
    "        if def_terms == \"\":\n",
    "            terms = \" AND \".join(rest_terms)\n",
    "        elif \" AND \".join(rest_terms) == \"\":\n",
    "            terms = def_terms\n",
    "        else:\n",
    "            terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= participial_prob:\n",
    "        # combine\n",
    "        def_terms = \" ; \".join(def_terms)\n",
    "        if def_terms == \"\":\n",
    "            terms = \" AND \".join(rest_terms)\n",
    "        elif \" AND \".join(rest_terms) == \"\":\n",
    "            terms = def_terms\n",
    "        else:\n",
    "            terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        return text, terms, _type\n",
    "    \n",
    "    # agent -> subject\n",
    "    # theme -> object\n",
    "    if \"theme\" in phi:\n",
    "        # only one object if it exists!\n",
    "        assert 1 == phi.split().count(\"theme\")\n",
    "        modify_subject = True if random.random() < 0.5 else False\n",
    "    else:\n",
    "        modify_subject = True\n",
    "    \n",
    "    # reindex!\n",
    "    def reindex_terms(terms, offset=1, distance=3):\n",
    "        new_terms = []\n",
    "        for term in terms:\n",
    "            new_term = []\n",
    "            for t in term.split():\n",
    "                if t.isnumeric() and int(t) > offset:\n",
    "                    new_term += [str(int(t)+distance)]\n",
    "                else:\n",
    "                    new_term += [t]\n",
    "            new_terms += [\" \".join(new_term)]\n",
    "        return new_terms\n",
    "    \n",
    "    # we need to decide whether to add to subj or obj.\n",
    "    second_text_pp = False\n",
    "    if modify_subject:\n",
    "        subject = text_split[0] if start_with_pp else text_split[1]\n",
    "        verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "        noun_pp = random.choice(list(noun_pool))\n",
    "        articles = 'a' if random.random() < 0.5 else 'the'\n",
    "        text_pp = [verb_pp_map[verb_pp], articles, noun_pp]\n",
    "        if articles == 'a':\n",
    "            pp_np_term =  [f\"{noun_pp} ( x _ 4 )\"]\n",
    "        elif articles == 'the':\n",
    "            pp_np_term = [f\"* {noun_pp} ( x _ 4 )\"]\n",
    "        pp_role_term = [f\"{subject} . acl . {verb_pp} ( x _ 1 , x _ 4 )\"]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            second_text_pp = True\n",
    "            second_verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "            second_noun_pp = random.choice(list(noun_pool))\n",
    "            second_articles = 'a' if random.random() < 0.5 else 'the'\n",
    "            text_pp += [verb_pp_map[second_verb_pp], second_articles, second_noun_pp]\n",
    "            if second_articles == 'a':\n",
    "                pp_np_term += [f\"{second_noun_pp} ( x _ 7 )\"]\n",
    "            elif second_articles == 'the':\n",
    "                pp_np_term += [f\"* {second_noun_pp} ( x _ 7 )\"]\n",
    "            pp_role_term += [f\"{noun_pp} . acl . {second_verb_pp} ( x _ 4 , x _ 7 )\"]\n",
    "            \n",
    "        def_terms = reindex_terms(def_terms, distance=6 if second_text_pp else 3)\n",
    "        rest_terms = reindex_terms(rest_terms, distance=6 if second_text_pp else 3)\n",
    "        \n",
    "        object_pos = 1\n",
    "        \n",
    "    else:\n",
    "        # locate the obj\n",
    "        assert \"x _ \" in object_entvar\n",
    "        object_pos = int(object_entvar.split()[-1])\n",
    "        _object = text_split[object_pos]\n",
    "        \n",
    "        verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "        noun_pp = random.choice(list(noun_pool))\n",
    "        articles = 'a' if random.random() < 0.5 else 'the'\n",
    "        text_pp = [verb_pp_map[verb_pp], articles, noun_pp]\n",
    "        if articles == 'a':\n",
    "            pp_np_term = [f\"{noun_pp} ( x _ {object_pos+3} )\"]\n",
    "        elif articles == 'the':\n",
    "            pp_np_term = [f\"* {noun_pp} ( x _ {object_pos+3} )\"]\n",
    "        pp_role_term = [f\"{_object} . acl . {verb_pp} ( x _ {object_pos} , x _ {object_pos+3} )\"]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            second_text_pp = True\n",
    "            second_verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "            second_noun_pp = random.choice(list(noun_pool))\n",
    "            second_articles = 'a' if random.random() < 0.5 else 'the'\n",
    "            text_pp += [verb_pp_map[second_verb_pp], second_articles, second_noun_pp]\n",
    "            if second_articles == 'a':\n",
    "                pp_np_term += [f\"{second_noun_pp} ( x _ {object_pos+6} )\"]\n",
    "            elif second_articles == 'the':\n",
    "                pp_np_term += [f\"* {second_noun_pp} ( x _ {object_pos+6} )\"]\n",
    "            pp_role_term += [f\"{noun_pp} . acl . {second_verb_pp} ( x _ {object_pos+3} , x _ {object_pos+6} )\"]\n",
    " \n",
    "        def_terms = reindex_terms(def_terms, object_pos, distance=6 if second_text_pp else 3)\n",
    "        rest_terms = reindex_terms(rest_terms, object_pos, distance=6 if second_text_pp else 3)\n",
    "    \n",
    "    for np_t in pp_np_term:\n",
    "        if np_t[0] == \"*\":\n",
    "            def_terms += [np_t]\n",
    "        else:\n",
    "            rest_terms += [np_t]\n",
    "    rest_terms += pp_role_term\n",
    "    text_split = text_split[:object_pos+1] + text_pp + text_split[object_pos+1:]\n",
    "    new_text = \" \".join(text_split)\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))   \n",
    "    rest_terms.sort(\n",
    "        key = lambda x: (\n",
    "            int(x.split()[-7]) if x.split()[-1] == \"acl\" else int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), \n",
    "            int(x.split()[-3]) if x.split()[-1] == \"acl\" else -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else new_text.split().index(x.split()[-2])\n",
    "        )\n",
    "    )  \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if second_text_pp:\n",
    "        return new_text, terms, \"subj_participle_verb_phrase_nested\" if modify_subject else \"obj_participle_verb_phrase_nested\"\n",
    "    else:\n",
    "        return new_text, terms, \"subj_participle_verb_phrase\" if modify_subject else \"obj_participle_verb_phrase\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "14c9f375",
   "metadata": {},
   "outputs": [],
   "source": [
    "participial_prob = 0.15\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "with open('./verb_pp_map.json', 'r') as openfile:\n",
    "    verb_pp_map = json.load(openfile)\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(\n",
    "    lambda x: collect_nv(*x), axis=1, result_type='expand'\n",
    ")\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(\n",
    "    lambda x: translate(*x), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d521dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"participle_verb\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f541db60",
   "metadata": {},
   "source": [
    "### Participial Verb Phrases (Easy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2fd9e356",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed)\n",
    "\n",
    "noun_pool = set()\n",
    "v_pool = set()\n",
    "def collect_nv(text, phi, _type):\n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    role_set = set([])\n",
    "    for d in data:\n",
    "        if d['type'] == 'role':\n",
    "            v_pool.add(d['pred'])\n",
    "        elif d['type'] == 'np':\n",
    "            noun_pool.add(d['pred'])\n",
    "            \n",
    "    return text, phi, _type\n",
    "\n",
    "def translate_regular(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "\n",
    "    return text, terms, _type\n",
    "\n",
    "def translate(text, phi, _type):\n",
    "    \n",
    "    if len(phi.split()) == 1:\n",
    "        return text, phi, _type\n",
    "    elif \"LAMBDA\" in phi:\n",
    "        return text, phi, _type\n",
    "    \n",
    "    # parse\n",
    "    text_split = text.split()\n",
    "    data = []    \n",
    "    conjs = re.split(r\"\\s*(?:AND|;)\\s*\", phi)\n",
    "    for conj in conjs: \n",
    "        if np_re.search(conj):\n",
    "            d = parse_np(conj)\n",
    "        elif pred_re.search(conj):\n",
    "            d = parse_pred(conj)\n",
    "        elif mod_re.search(conj):\n",
    "            d = parse_mod(conj)\n",
    "        else:\n",
    "            raise Exception(f\"Conjunct could not be parsed: {conj}\")\n",
    "        data.append(d)\n",
    "    \n",
    "    # collect\n",
    "    def_terms = []\n",
    "    role_terms = []\n",
    "    nmod_terms = []\n",
    "    role_set = set([])\n",
    "    object_entvar = None\n",
    "    for d in data:\n",
    "        if d['type'] == 'np':\n",
    "            if d['definiteness'] == '*':\n",
    "                def_terms += [f\"* {d['pred']} ( {d['entvar']} )\"]\n",
    "            else:\n",
    "                role_terms += [f\"{d['pred']} ( {d['entvar']} )\"]\n",
    "        if d['type'] == 'role':\n",
    "            role_terms += [f\"{d['pred']} . {d['role']} ( {d['eventvar']} , {d['entvar']} )\"]\n",
    "            if d['role'] == \"theme\":\n",
    "                object_entvar = d['entvar']\n",
    "                \n",
    "            role_set.add(d['role'])\n",
    "        elif d['type'] == 'mod':\n",
    "            assert \"x _\" in d['e1']\n",
    "            role_terms += [f\"{d['nppred']} . nmod . {d['pred']} ( {d['e1']} , {d['e2']} )\"]\n",
    "            \n",
    "    # sort def_terms\n",
    "    def_terms = [*set(def_terms)]\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))    \n",
    "    rest_terms = role_terms\n",
    "    \n",
    "    start_with_pp = False if text_split[0] in {'A', 'The'} else True\n",
    "    \n",
    "    if start_with_pp or (object_entvar is not None and \"x _ \" not in object_entvar):\n",
    "        # combine\n",
    "        def_terms = \" ; \".join(def_terms)\n",
    "        if def_terms == \"\":\n",
    "            terms = \" AND \".join(rest_terms)\n",
    "        elif \" AND \".join(rest_terms) == \"\":\n",
    "            terms = def_terms\n",
    "        else:\n",
    "            terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        return text, terms, _type\n",
    "    \n",
    "    if random.random() >= participial_prob:\n",
    "        # combine\n",
    "        def_terms = \" ; \".join(def_terms)\n",
    "        if def_terms == \"\":\n",
    "            terms = \" AND \".join(rest_terms)\n",
    "        elif \" AND \".join(rest_terms) == \"\":\n",
    "            terms = def_terms\n",
    "        else:\n",
    "            terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "        return text, terms, _type\n",
    "    \n",
    "    # agent -> subject\n",
    "    # theme -> object\n",
    "    if \"theme\" in phi:\n",
    "        # only one object if it exists!\n",
    "        assert 1 == phi.split().count(\"theme\")\n",
    "        modify_subject = True if random.random() < 0.5 else False\n",
    "    else:\n",
    "        modify_subject = True\n",
    "    \n",
    "    # reindex!\n",
    "    def reindex_terms(terms, offset=1, distance=3):\n",
    "        new_terms = []\n",
    "        for term in terms:\n",
    "            new_term = []\n",
    "            for t in term.split():\n",
    "                if t.isnumeric() and int(t) > offset:\n",
    "                    new_term += [str(int(t)+distance)]\n",
    "                else:\n",
    "                    new_term += [t]\n",
    "            new_terms += [\" \".join(new_term)]\n",
    "        return new_terms\n",
    "    \n",
    "    # we need to decide whether to add to subj or obj.\n",
    "    second_text_pp = False\n",
    "    if modify_subject:\n",
    "        subject = text_split[0] if start_with_pp else text_split[1]\n",
    "        verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "        noun_pp = random.choice(list(noun_pool))\n",
    "        articles = 'a' if random.random() < 0.5 else 'the'\n",
    "        text_pp = [verb_pp_map[verb_pp], articles, noun_pp]\n",
    "        if articles == 'a':\n",
    "            pp_np_term =  [f\"{noun_pp} ( x _ 4 )\"]\n",
    "        elif articles == 'the':\n",
    "            pp_np_term = [f\"* {noun_pp} ( x _ 4 )\"]\n",
    "        pp_role_term = [f\"{subject} . nmod . {verb_pp} ( x _ 1 , x _ 4 )\"]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            second_text_pp = True\n",
    "            second_verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "            second_noun_pp = random.choice(list(noun_pool))\n",
    "            second_articles = 'a' if random.random() < 0.5 else 'the'\n",
    "            text_pp += [verb_pp_map[second_verb_pp], second_articles, second_noun_pp]\n",
    "            if second_articles == 'a':\n",
    "                pp_np_term += [f\"{second_noun_pp} ( x _ 7 )\"]\n",
    "            elif second_articles == 'the':\n",
    "                pp_np_term += [f\"* {second_noun_pp} ( x _ 7 )\"]\n",
    "            pp_role_term += [f\"{noun_pp} . nmod . {second_verb_pp} ( x _ 4 , x _ 7 )\"]\n",
    "            \n",
    "        def_terms = reindex_terms(def_terms, distance=6 if second_text_pp else 3)\n",
    "        rest_terms = reindex_terms(rest_terms, distance=6 if second_text_pp else 3)\n",
    "        \n",
    "        object_pos = 1\n",
    "        \n",
    "    else:\n",
    "        # locate the obj\n",
    "        assert \"x _ \" in object_entvar\n",
    "        object_pos = int(object_entvar.split()[-1])\n",
    "        _object = text_split[object_pos]\n",
    "        \n",
    "        verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "        noun_pp = random.choice(list(noun_pool))\n",
    "        articles = 'a' if random.random() < 0.5 else 'the'\n",
    "        text_pp = [verb_pp_map[verb_pp], articles, noun_pp]\n",
    "        if articles == 'a':\n",
    "            pp_np_term = [f\"{noun_pp} ( x _ {object_pos+3} )\"]\n",
    "        elif articles == 'the':\n",
    "            pp_np_term = [f\"* {noun_pp} ( x _ {object_pos+3} )\"]\n",
    "        pp_role_term = [f\"{_object} . nmod . {verb_pp} ( x _ {object_pos} , x _ {object_pos+3} )\"]\n",
    "        \n",
    "        if random.random() < 0.5:\n",
    "            second_text_pp = True\n",
    "            second_verb_pp = random.choice(list(verb_pp_map.keys()))\n",
    "            second_noun_pp = random.choice(list(noun_pool))\n",
    "            second_articles = 'a' if random.random() < 0.5 else 'the'\n",
    "            text_pp += [verb_pp_map[second_verb_pp], second_articles, second_noun_pp]\n",
    "            if second_articles == 'a':\n",
    "                pp_np_term += [f\"{second_noun_pp} ( x _ {object_pos+6} )\"]\n",
    "            elif second_articles == 'the':\n",
    "                pp_np_term += [f\"* {second_noun_pp} ( x _ {object_pos+6} )\"]\n",
    "            pp_role_term += [f\"{noun_pp} . nmod . {second_verb_pp} ( x _ {object_pos+3} , x _ {object_pos+6} )\"]\n",
    " \n",
    "        def_terms = reindex_terms(def_terms, object_pos, distance=6 if second_text_pp else 3)\n",
    "        rest_terms = reindex_terms(rest_terms, object_pos, distance=6 if second_text_pp else 3)\n",
    "    \n",
    "    for np_t in pp_np_term:\n",
    "        if np_t[0] == \"*\":\n",
    "            def_terms += [np_t]\n",
    "        else:\n",
    "            rest_terms += [np_t]\n",
    "    rest_terms += pp_role_term\n",
    "    text_split = text_split[:object_pos+1] + text_pp + text_split[object_pos+1:]\n",
    "    new_text = \" \".join(text_split)\n",
    "    def_terms.sort(key = lambda x: int(x.split()[-2]))   \n",
    "    rest_terms.sort(\n",
    "        key = lambda x: (\n",
    "            int(x.split()[-7]) if x.split()[-1] == \"nmod\" else int(x.split()[-2]) if x.split()[-5] == \"(\" else int(x.split()[-6]) if x.split()[-2].isnumeric() else int(x.split()[-4]), \n",
    "            int(x.split()[-3]) if x.split()[-1] == \"nmod\" else -1 if x.split()[-5] == \"(\" else int(x.split()[-2]) if x.split()[-2].isnumeric() else new_text.split().index(x.split()[-2])\n",
    "        )\n",
    "    )  \n",
    "    # combine\n",
    "    def_terms = \" ; \".join(def_terms)\n",
    "    if def_terms == \"\":\n",
    "        terms = \" AND \".join(rest_terms)\n",
    "    elif \" AND \".join(rest_terms) == \"\":\n",
    "        terms = def_terms\n",
    "    else:\n",
    "        terms = def_terms + \" ; \" + \" AND \".join(rest_terms)\n",
    "    \n",
    "    if second_text_pp:\n",
    "        return new_text, terms, \"subj_participle_verb_phrase_nested\" if modify_subject else \"obj_participle_verb_phrase_nested\"\n",
    "    else:\n",
    "        return new_text, terms, \"subj_participle_verb_phrase\" if modify_subject else \"obj_participle_verb_phrase\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51b9eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "participial_prob = 0.15\n",
    "train_df = pd.read_csv(\"./cogs/train.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "dev_df = pd.read_csv(\"./cogs/dev.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "test_df = pd.read_csv(\"./cogs/test.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "gen_df = pd.read_csv(\"./cogs/gen.tsv\", sep=\"\\t\", names=['sentence', 'LF', 'type'])\n",
    "\n",
    "with open('./verb_pp_map.json', 'r') as openfile:\n",
    "    verb_pp_map = json.load(openfile)\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(\n",
    "    lambda x: collect_nv(*x), axis=1, result_type='expand'\n",
    ")\n",
    "train_df[['sentence', 'LF', 'type']] = train_df[['sentence', 'LF', 'type']].apply(\n",
    "    lambda x: translate(*x), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "dev_df[['sentence', 'LF', 'type']] = dev_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "test_df[['sentence', 'LF', 'type']] = test_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')\n",
    "gen_df[['sentence', 'LF', 'type']] = gen_df[['sentence', 'LF', 'type']].apply(lambda x: translate_regular(*x), axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a182c7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_postfix = \"participle_verb_easy\"\n",
    "train_df.to_csv(f'./cogs_{dataset_postfix}/train.tsv', sep='\\t', index=False, header=False)\n",
    "dev_df.to_csv(f'./cogs_{dataset_postfix}/dev.tsv', sep='\\t', index=False, header=False)\n",
    "test_df.to_csv(f'./cogs_{dataset_postfix}/test.tsv', sep='\\t', index=False, header=False)\n",
    "gen_df.to_csv(f'./cogs_{dataset_postfix}/gen.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c656cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
